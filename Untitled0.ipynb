{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SajalSinha/SajalSinha/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stanfordcorenlp"
      ],
      "metadata": {
        "id": "kcp8-qU8RAay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/fzschornack/aspect-transfer-blstm-crf"
      ],
      "metadata": {
        "id": "Q08DZ41dYDdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/software/stanford-corenlp-latest.zip"
      ],
      "metadata": {
        "id": "KO1xj_J3Yf-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip stanford-corenlp-latest.zip"
      ],
      "metadata": {
        "id": "kD2yBdmcZC9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "fpZwOOO_aF4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "JiX0Y59CZnL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xml.etree.ElementTree\n",
        "from stanfordcorenlp import StanfordCoreNLP\n",
        "import string\n",
        "import re\n",
        "import itertools\n",
        "import csv\n",
        "import json\n",
        "from os import path\n",
        "import numpy as np\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "\n",
        "\n",
        "# In[2]:\n",
        "\n",
        "# POS Tags binary vectors\n",
        "POS_VECTORS = {'NN': np.array([1., 0., 0., 0., 0., 0.]),\n",
        "              'NNS': np.array([1., 0., 0., 0., 0., 0.]),\n",
        "              'NNP': np.array([1., 0., 0., 0., 0., 0.]),\n",
        "              'NNPS': np.array([1., 0., 0., 0., 0., 0.]),\n",
        "              'VB': np.array([0., 1., 0., 0., 0., 0.]),\n",
        "              'VBD': np.array([0., 1., 0., 0., 0., 0.]),\n",
        "              'VBG': np.array([0., 1., 0., 0., 0., 0.]),\n",
        "              'VBN': np.array([0., 1., 0., 0., 0., 0.]),\n",
        "              'VBP': np.array([0., 1., 0., 0., 0., 0.]),\n",
        "              'VBZ': np.array([0., 1., 0., 0., 0., 0.]),\n",
        "              'JJ': np.array([0., 0., 1., 0., 0., 0.]),\n",
        "              'JJR': np.array([0., 0., 1., 0., 0., 0.]),\n",
        "              'JJS': np.array([0., 0., 1., 0., 0., 0.]),\n",
        "              'RB': np.array([0., 0., 0., 1., 0., 0.]),\n",
        "              'RBR': np.array([0., 0., 0., 1., 0., 0.]),\n",
        "              'RBS': np.array([0., 0., 0., 1., 0., 0.]),\n",
        "              'IN': np.array([0., 0., 0., 0., 1., 0.]),\n",
        "              'CC': np.array([0., 0., 0., 0., 0., 1.])\n",
        "             }\n",
        "\n",
        "# Stanford core nlp list of tags\n",
        "POS_TAGS = { '\\'\\'': 0, ',': 1, '.': 2, ':': 3, '``': 4, 'CC': 5, 'CD': 6, 'DT': 7, 'EX': 8, 'FW': 9, 'IN': 10, 'JJ': 11, 'JJR': 12, 'JJS': 13, 'LS': 14, 'MD': 15, 'NN': 16, 'NNP': 17, 'NNPS': 18, 'NNS': 19, 'PDT': 20, 'POS': 21, 'PRP': 22, 'PRP$': 23, 'RB': 24, 'RBR': 25, 'RBS': 26, 'RP': 27, 'SYM': 28, 'TO': 29, 'UH': 30, 'VB': 31, 'VBD': 32, 'VBG': 33, 'VBN': 34, 'VBP': 35, 'VBZ': 36, 'WDT': 37, 'WP': 38, 'WP$': 39, 'WRB': 40 } \n",
        "\n",
        "\n",
        "# In[3]:\n",
        "\n",
        "# Load sentic2vec\n",
        "def load_sentic2vec_model():\n",
        "    print(\"Loading sentic2vec Model\")\n",
        "    sentic2vec = {}\n",
        "    with open('sentic2vec.csv', encoding='ISO-8859-1') as csvfile:\n",
        "        reader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
        "        for row in reader:\n",
        "            head, *tail = row\n",
        "            tail = np.array([float(i) for i in tail])\n",
        "            sentic2vec.update({head: tail})\n",
        "    print(\"Done.\", len(sentic2vec), \" words loaded!\")\n",
        "    return sentic2vec\n",
        "\n",
        "def load_glove_model():\n",
        "    glove_file_path = \"/content/drive/MyDrive/AspectExtraction/glove.840B.300d.txt\"\n",
        "    print(\"Loading Glove Model\")\n",
        "    f = open(glove_file_path,'r', encoding=\"utf8\")\n",
        "    model = {}\n",
        "    for line in f:\n",
        "        split_line = line.split()\n",
        "        word = split_line[0]\n",
        "        \n",
        "        embedding = np.array([float(val) for val in split_line[1:] if val not in [',','-','.','name@domain.com','Killerseats.com','mylot.com']])\n",
        "        model[word] = embedding\n",
        "    print(\"Done.\", len(model), \" words loaded!\")\n",
        "    return model\n",
        "\n",
        "\n",
        "# In[4]:\n",
        "\n",
        "def build_dict(aspects):\n",
        "    myDict = {}\n",
        "    for aspect in aspects:\n",
        "        begin = int(aspect[\"from\"])\n",
        "        terms = aspect[\"term\"].split(\" \")\n",
        "        position = begin + len(terms[0])\n",
        "        myDict[begin] = 1\n",
        "        for term in terms[1:]:\n",
        "            position += 1\n",
        "            myDict[position] = 2\n",
        "            position = position + len(term)\n",
        "    return myDict\n",
        "\n",
        "def get_pos_vector(text_pos_tags, word, word_before_clean):\n",
        "    original_word = __get_original_word(word)\n",
        "\n",
        "    for i in range(len(text_pos_tags)):\n",
        "        token = text_pos_tags[i][0]\n",
        "        token_tag = text_pos_tags[i][1]\n",
        "        \n",
        "        if token.lower() == word or token.lower() == word_before_clean.lower() or token.lower() in original_word:\n",
        "            del text_pos_tags[i]\n",
        "            break\n",
        "    \n",
        "    pos_vector = POS_VECTORS.get(token_tag, np.array([0., 0., 0., 0., 0., 0.])) # get POS binary vector\n",
        "    \n",
        "    return text_pos_tags, pos_vector\n",
        "    \n",
        "\n",
        "def __remove_punctuation(word):\n",
        "    return word.translate(str.maketrans('','',string.punctuation))\n",
        "\n",
        "def __get_original_word(word):\n",
        "    if word == \"is\": return \"'s\"\n",
        "    elif word == \"have\": return \"'ve\"\n",
        "    elif word == \"shall\": return \"sha\"\n",
        "    elif word == \"will\": return [\"wo\", \"'ll\"]\n",
        "    elif word == \"can\": return \"ca\"\n",
        "    elif word == \"not\": return \"n't\"\n",
        "    elif word == \"am\": return \"'m\"\n",
        "    elif word == \"are\": return \"'re\"\n",
        "    elif word == \"would\": return \"'d\"\n",
        "    else: return word\n",
        "    \n",
        "def clean_text(word, label):\n",
        "    word = re.sub(r\"\\'s\", \" is\", word)\n",
        "    word = re.sub(r\"\\'ve\", \" have\", word)\n",
        "    word = re.sub(r\"shan't\", \"shall not\", word)\n",
        "    word = re.sub(r\"won't\", \"will not\", word)\n",
        "    word = re.sub(r\"can't\", \"can not\", word)\n",
        "    word = re.sub(r\"n't\", \" not\", word)\n",
        "    word = re.sub(r\"i'm\", \"i am\", word)\n",
        "    word = re.sub(r\"\\'re\", \" are\", word)\n",
        "    word = re.sub(r\"\\'d\", \" would\", word)\n",
        "    word = re.sub(r\"\\'ll\", \" will\", word)\n",
        "    word = __remove_punctuation(word)\n",
        "    word = word.lower()\n",
        "    return [(w, label) for w in word.split(\" \")]\n",
        "\n",
        "\n",
        "\n",
        "# In[5]:\n",
        "\n",
        "def create_inputs(words_n_labels_n_pos_vectors, embedding_model, use_pos_vectors):\n",
        "    train_lstm = []\n",
        "    train_5_gram = []\n",
        "    labels = []\n",
        "    embedding_total_size = 300\n",
        "    total_words = len(words_n_labels_n_pos_vectors)\n",
        "    total_zeros = 0\n",
        "\n",
        "    if (use_pos_vectors == True):\n",
        "        embedded_words = [np.concatenate((embedding_model.get(word, np.zeros(300)), pos_vector)) for (word, label, pos_vector) in words_n_labels_n_pos_vectors]\n",
        "        total_zeros = sum([0 if word in embedding_model else 1 for (word, label, pos_vector) in words_n_labels_n_pos_vectors])\n",
        "        embedding_total_size = 306\n",
        "    else:\n",
        "        words_n_labels = [(t[0], t[1]) for t in words_n_labels_n_pos_vectors]\n",
        "        embedded_words = [embedding_model.get(word, np.zeros(300)) for (word, label) in words_n_labels]\n",
        "        total_zeros = sum([0 if word in embedding_model else 1 for (word, label) in words_n_labels])\n",
        "        \n",
        "\n",
        "    for i in range(len(words_n_labels_n_pos_vectors)):\n",
        "        label = words_n_labels_n_pos_vectors[i][1]\n",
        "        \n",
        "        # create one embedding matrix for each example: [pre-word1, pre-word2, main-word, pos-word1, pos-word2]\n",
        "        embedding_matrix = np.zeros((5, embedding_total_size))\n",
        "\n",
        "        embedding_matrix[0] = embedded_words[i-2] if i - 2 >= 0 else np.zeros(embedding_total_size) # pre-word1\n",
        "        embedding_matrix[1] = embedded_words[i-1] if i - 1 >= 0 else np.zeros(embedding_total_size) # pre-word2\n",
        "        embedding_matrix[2] = embedded_words[i] # main-word\n",
        "        embedding_matrix[3] = embedded_words[i+1] if i + 1 < len(words_n_labels_n_pos_vectors) else np.zeros(embedding_total_size) # pos-word1\n",
        "        embedding_matrix[4] = embedded_words[i+2] if i + 2 < len(words_n_labels_n_pos_vectors) else np.zeros(embedding_total_size) # pos-word2\n",
        "\n",
        "        # 5-gram input format\n",
        "        train_5_gram.append(embedding_matrix)\n",
        "\n",
        "        # lstm input format\n",
        "        train_lstm.append(embedded_words[i]) # main-word\n",
        "\n",
        "        labels.append(label)\n",
        "        \n",
        "    return train_lstm, train_5_gram, labels, total_words, total_zeros\n",
        "\n",
        "\n",
        "# In[22]:\n",
        "\n",
        "def read_xml_file(file_path, embedding_type='glove', use_pos_vectors=False):\n",
        "    TEST_IDS = []\n",
        "    if (file_path == 'data/Laptops_Train.xml'):\n",
        "        TEST_IDS = [2128,81,89,353,347,1813,655,1615,1670,2443,764,177,3012,1479,2937,439,2925,929,2077,225,2756,2863,1393,1837,921,2817,1896,341,36,990,2275,2908,214,2777,467,47,361,168,2642,1000,2135,1733,1822,3064,1646,2988,2828,121,1967,1776,2626,2967,1800,2993,1037,1270,1834,2451,605,1900,2871,844,1634,200,416,68,2746,282,1532,2238,692,78,2245,415,2710,1671,2703,66,285,1609,2661,1577,1287,2523,2785,832,2037,1429,2130,1446,2259,2695,1506,3047,1042,2151,1732,1063,271,1674]\n",
        "    elif (file_path == 'data/Restaurants_Train.xml'):\n",
        "        TEST_IDS = [813,1579,2707,3126,2882,1609,3018,3292,2041,2609,1194,3440,870,2507,586,3081,1884,2077,201,3049,1242,2211,3343,2141,83,381,3315,1709,193,2459,1293,2105,2152,3358,2751,882,1124,3546,3020,746,3170,2171,2811,762,339,217,3478,1180,3372,776,3208,2673,1380,3641,2806,2630,924,3411,2164,3077,745,1273,179,2460,3045,1698,109,1057,2186,3471,1401,1112,3368,3495,2451,948,2202,2740,537,1548,134,2708,1455,2591,1593,1769,3575,1105,2575,742,1471,1656,2769,3681,2205,1159,628,2912,3188,3041]\n",
        "\n",
        "    NLP = StanfordCoreNLP(path.relpath(r'/content/stanford-corenlp-4.4.0'))\n",
        "    \n",
        "    embedding_model = {}\n",
        "    if (embedding_type == 'glove'):\n",
        "        embedding_model = load_glove_model()\n",
        "    elif (embedding_type == 'sentic'):\n",
        "        embedding_model = load_sentic2vec_model()\n",
        "\n",
        "    train_list_5_gram = []\n",
        "    train_list_neural_net = []\n",
        "    labels_list_neural_net = []\n",
        "    train_list_lstm = []\n",
        "    labels_list_lstm = []\n",
        "    sentences_length_lstm = []\n",
        "    original_sentences = []\n",
        "    total_words = 0\n",
        "    total_zeros = 0\n",
        "\n",
        "    # parse\n",
        "    e = xml.etree.ElementTree.parse(file_path).getroot()\n",
        "    for sentence in e.iter('sentence'):\n",
        "        id_ = sentence.get('id')\n",
        "        \n",
        "        # remove test data from train data\n",
        "        if int(id_) in TEST_IDS and (file_path == 'data/Laptops_Train.xml' \n",
        "                                     or file_path == 'data/Restaurants_Train.xml'): continue\n",
        "        \n",
        "        text = sentence.find('text').text\n",
        "        aspects = [{\"term\": a.get('term'), \n",
        "                    \"polarity\": a.get('polarity'), \n",
        "                    \"from\": a.get('from'), \n",
        "                    \"to\": a.get('to')} for a in sentence.iter('aspectTerm')]\n",
        "\n",
        "        words_n_labels_n_pos_vectors = []\n",
        "        aspectsDict = build_dict(aspects)\n",
        "\n",
        "        text_pos_tags = NLP.pos_tag(text) # get POSTags using stanford core NLP\n",
        "\n",
        "        i = 0\n",
        "        while i < len(text):\n",
        "            if i in aspectsDict:\n",
        "                label = aspectsDict[i]\n",
        "            else:\n",
        "                label = 0\n",
        "\n",
        "            word = ''\n",
        "            while i < len(text) and text[i] != ' ':\n",
        "                word += text[i]\n",
        "                i += 1\n",
        "                        \n",
        "            clean_words_n_labels = clean_text(word, label)\n",
        "\n",
        "            # get POS vectors\n",
        "            for (clean_word, label) in clean_words_n_labels:\n",
        "                text_pos_tags, pos_vector = get_pos_vector(text_pos_tags, clean_word, word) # update 'text_pos_tags' and get 'pos_vector'\n",
        "                word_label_pos_vector = (clean_word, label, pos_vector)\n",
        "                \n",
        "                words_n_labels_n_pos_vectors.append(word_label_pos_vector)\n",
        "\n",
        "            i += 1\n",
        "\n",
        "        train_lstm, train_5_gram, labels, num_words, num_zeros = create_inputs(words_n_labels_n_pos_vectors, embedding_model, use_pos_vectors)\n",
        "        \n",
        "        train_list_neural_net += train_5_gram\n",
        "        labels_list_neural_net += labels\n",
        "        train_list_5_gram.append(train_5_gram)\n",
        "        train_list_lstm.append(train_lstm)\n",
        "        labels_list_lstm.append(labels)\n",
        "        \n",
        "        total_words += num_words\n",
        "        total_zeros += num_zeros\n",
        "        \n",
        "        sentences_length_lstm.append(len(words_n_labels_n_pos_vectors))\n",
        "        original_sentences.append([word for (word, label, pos_vector) in words_n_labels_n_pos_vectors])\n",
        "\n",
        "\n",
        "    print(\"Total words in the set:\", total_words)\n",
        "    print(\"Total words not recognized by the embedding:\", total_zeros)\n",
        "\n",
        "    NLP.close()\n",
        "    del embedding_model\n",
        "    \n",
        "    return train_list_neural_net, labels_list_neural_net, train_list_lstm, labels_list_lstm, train_list_5_gram, sentences_length_lstm, original_sentences\n",
        "\n",
        "\n",
        "# In[6]:\n",
        "\n",
        "def read_pos_tagging_file(file_path):\n",
        "    train_list_5_gram = []\n",
        "    train_list_lstm = []\n",
        "    labels_list_lstm = []\n",
        "    sentences_length_lstm = []\n",
        "    original_sentences = []\n",
        "    total_words = 0\n",
        "    total_zeros = 0\n",
        "    \n",
        "    embedding_model = load_glove_model()\n",
        "    \n",
        "    f = open(file_path,'r')\n",
        "    for line in f:\n",
        "        token_tags = json.loads(line)['tags']\n",
        "        words_labels = [(tk_tg['tk'].lower(), POS_TAGS[tk_tg['tg']]) for tk_tg in token_tags]\n",
        "\n",
        "        train_lstm, train_5_gram, labels, num_words, num_zeros = create_inputs(words_labels, embedding_model, False)\n",
        "\n",
        "        train_list_5_gram.append(train_5_gram)\n",
        "        train_list_lstm.append(train_lstm)\n",
        "        labels_list_lstm.append(labels)\n",
        "        \n",
        "        total_words += num_words\n",
        "        total_zeros += num_zeros\n",
        "        \n",
        "        sentences_length_lstm.append(len(words_labels))\n",
        "    \n",
        "    print(\"Total words in the set:\", total_words)\n",
        "    print(\"Total words not recognized by the embedding:\", total_zeros)\n",
        "    \n",
        "    del embedding_model\n",
        "    \n",
        "    return train_list_lstm, labels_list_lstm, train_list_5_gram, sentences_length_lstm\n"
      ],
      "metadata": {
        "id": "lZz3mX9sL10r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecZbIS7eLhJ7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "import pandas as pd\n",
        "\n",
        "# model with pre-trained weights that are going to be transfered (transfer learning)\n",
        "MODEL_PATH = 'model_bi_lstm_crf_laptops_220k_50_75'\n",
        "\n",
        "_, _, x_orig, y_orig, _, sequence_length_orig, original_orig_sentences = read_xml_file('data/Laptops_Train.xml')\n",
        "_, _, x_test, y_test, _, sequence_length_test, original_test_sentences = read_xml_file('data/laptops-trial.xml')\n",
        "\n",
        "def pad(sentence, max_length, is_label, input_size=0):\n",
        "    pad_len = max_length - len(sentence)\n",
        "    padding = np.zeros(pad_len) if is_label == True else np.zeros((pad_len, input_size))\n",
        "    return np.concatenate((sentence, padding))\n",
        "\n",
        "def pad_5_gram(sentence, max_length, input_size=0):\n",
        "    pad_len = max_length - len(sentence)\n",
        "    padding = np.zeros((pad_len, 5, input_size)) # 5-gram\n",
        "    return np.concatenate((sentence, padding))\n",
        "\n",
        "def batch(data, labels, sequence_lengths, batch_size, input_size):\n",
        "    n_batch = int(math.ceil(len(data) / batch_size))\n",
        "    index = 0\n",
        "    for _ in range(n_batch):\n",
        "        batch_sequence_lengths = np.array(sequence_lengths[index: index + batch_size])\n",
        "        batch_length = np.array(max(batch_sequence_lengths)) # max length in batch\n",
        "        batch_data = np.array([pad(x, batch_length, False, input_size) for x in data[index: index + batch_size]]) # pad data\n",
        "        batch_labels = np.array([pad(x, batch_length, True) for x in labels[index: index + batch_size]]) # pad labels\n",
        "        index += batch_size\n",
        "        \n",
        "        # Reshape input data to be suitable for LSTMs.\n",
        "        batch_data = batch_data.reshape(-1, batch_length, input_size)\n",
        "        \n",
        "        \n",
        "        yield batch_data, batch_labels, batch_length, batch_sequence_lengths\n",
        "\n",
        "\n",
        "# Bidirectional LSTM + CRF model.\n",
        "learning_rate = 0.001\n",
        "input_size = 300\n",
        "num_units = 256 # the number of units in the LSTM cell\n",
        "number_of_classes = 3\n",
        "\n",
        "input_data = tf.placeholder(tf.float32, [None, None, input_size], name=\"input_data\") # shape = (batch, batch_seq_len, input_size)\n",
        "labels = tf.placeholder(tf.int32, shape=[None, None], name=\"labels\") # shape = (batch, sentence)\n",
        "batch_sequence_length = tf.placeholder(tf.int32) # max sequence length in batch\n",
        "original_sequence_lengths = tf.placeholder(tf.int32, [None])\n",
        "\n",
        "# Scope is mandatory to use LSTMCell (https://github.com/tensorflow/tensorflow/issues/799).\n",
        "with tf.name_scope(\"BiLSTM\"):\n",
        "    with tf.variable_scope('forward'):\n",
        "        lstm_fw_cell = tf.nn.rnn_cell.LSTMCell(num_units, forget_bias=1.0, state_is_tuple=True, name=\"fw_cell\")\n",
        "    with tf.variable_scope('backward'):\n",
        "        lstm_bw_cell = tf.nn.rnn_cell.LSTMCell(num_units, forget_bias=1.0, state_is_tuple=True, name=\"bw_cell\")\n",
        "    (output_fw, output_bw), states = tf.nn.bidirectional_dynamic_rnn(cell_fw=lstm_fw_cell, \n",
        "                                                                     cell_bw=lstm_bw_cell, \n",
        "                                                                     inputs=input_data,\n",
        "                                                                     sequence_length=original_sequence_lengths, \n",
        "                                                                     dtype=tf.float32,\n",
        "                                                                     scope=\"BiLSTM\")\n",
        "\n",
        "# As we have a Bi-LSTM, we have two outputs which are not connected, so we need to merge them.\n",
        "outputs = tf.concat([output_fw, output_bw], axis=2)\n",
        "\n",
        "# Fully connected layer.\n",
        "W = tf.get_variable(name=\"W\", shape=[2 * num_units, number_of_classes],\n",
        "                dtype=tf.float32, initializer=tf.truncated_normal_initializer(mean=0.0, stddev=0.01))\n",
        "\n",
        "b = tf.get_variable(name=\"b\", shape=[number_of_classes], dtype=tf.float32,\n",
        "                initializer=tf.truncated_normal_initializer(mean=0.0, stddev=0.01))\n",
        "\n",
        "outputs_flat = tf.reshape(outputs, [-1, 2 * num_units])\n",
        "pred = tf.matmul(outputs_flat, W) + b\n",
        "scores = tf.reshape(pred, [-1, batch_sequence_length, number_of_classes])\n",
        "\n",
        "# Linear-CRF.\n",
        "log_likelihood, transition_params = tf.contrib.crf.crf_log_likelihood(scores, labels, original_sequence_lengths)\n",
        "\n",
        "loss = tf.reduce_mean(-log_likelihood)\n",
        "\n",
        "# Compute the viterbi sequence and score (used for prediction and test time).\n",
        "viterbi_sequence, viterbi_score = tf.contrib.crf.crf_decode(scores, transition_params, original_sequence_lengths)\n",
        "\n",
        "# Training ops.\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "train_op = optimizer.minimize(loss)\n",
        "\n",
        "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
        "\n",
        "\n",
        "# transfer learning: load only part of the layers\n",
        "variables_to_restore = [var for var in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='BiLSTM/fw')\n",
        "                        if var.name.startswith('BiLSTM/fw/fw_cell')] + [var for var in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='BiLSTM/bw')\n",
        "                        if var.name.startswith('BiLSTM/bw/bw_cell')]\n",
        "\n",
        "# https://stackoverflow.com/questions/45617026/tensorflow-transfer-learning-how-to-load-part-of-layers-from-one-checkpoint-file\n",
        "# Save/restore variables from the old model (transfer learning).\n",
        "saver = tf.train.Saver(variables_to_restore)\n",
        "\n",
        "\n",
        "\n",
        "def get_valid_labels_predictions(batch_labels, tf_viterbi_sequence, batch_sequence_lengths):\n",
        "    valid_labels = []\n",
        "    valid_predictions = []\n",
        "    for i in range(len(batch_labels)):\n",
        "        l = batch_sequence_lengths[i]\n",
        "        for j in range(l):\n",
        "            valid_labels.append(batch_labels[i][j])\n",
        "            valid_predictions.append(tf_viterbi_sequence[i][j])\n",
        "            \n",
        "    return valid_labels, valid_predictions\n",
        "\n",
        "\n",
        "def count_errors(valid_labels, valid_predictions):\n",
        "\n",
        "    # Post-processing\n",
        "    for i in range(len(valid_labels)):\n",
        "        if valid_labels[i] == 2:\n",
        "            valid_labels[i-1] = 2\n",
        "    \n",
        "    zeros_idx = []\n",
        "    singular_idx = []\n",
        "    compound_idx = []\n",
        "\n",
        "    i = 0\n",
        "    while i < len(valid_labels):\n",
        "        if valid_labels[i] == 0:\n",
        "            zeros_idx.append(i)\n",
        "            i += 1\n",
        "        elif valid_labels[i] == 1:\n",
        "            singular_idx.append(i)\n",
        "            i += 1\n",
        "        else:\n",
        "            compound = []\n",
        "            compound.append(i)\n",
        "            i += 1\n",
        "            while valid_labels[i] == 2:\n",
        "                compound.append(i)\n",
        "                i += 1\n",
        "            compound_idx.append(compound)\n",
        "\n",
        "\n",
        "    zeros_err = 0\n",
        "    for i in range(len(zeros_idx)):\n",
        "        if valid_predictions[zeros_idx[i]] != 0:\n",
        "            zeros_err += 1\n",
        "\n",
        "    singular_err = 0\n",
        "    for i in range(len(singular_idx)):\n",
        "        if valid_predictions[singular_idx[i]] == 0:\n",
        "            singular_err += 1\n",
        "\n",
        "    compound_err = 0\n",
        "    for i in range(len(compound_idx)):\n",
        "        for j in range(len(compound_idx[i])):\n",
        "            if valid_predictions[compound_idx[i][j]] == 0:\n",
        "                compound_err += 1\n",
        "                break\n",
        "\n",
        "    print(\"errors_count: \",zeros_err, singular_err, compound_err)\n",
        "    total_aspects = len(singular_idx) + len(compound_idx)\n",
        "    try:\n",
        "        recall = (total_aspects - (singular_err + compound_err)) / total_aspects\n",
        "        precision = (total_aspects - (singular_err + compound_err)) / ((total_aspects - (singular_err + compound_err)) + zeros_err)\n",
        "        f1_score = 2 * (precision*recall) / (precision + recall)\n",
        "        print(\"P: %.4f%%\" % precision, \"R: %.4f%%\" % recall, \"F1: %.4f%%\" % f1_score)\n",
        "    except ZeroDivisionError:\n",
        "        print(\"zero division\")\n",
        "\n",
        "\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.per_process_gpu_memory_fraction = 0.2\n",
        "\n",
        "training_epochs = 1030\n",
        "runs = 10\n",
        "batch_size = 128\n",
        "\n",
        "# Training the model.\n",
        "with tf.Session(config=config) as session:\n",
        "    \n",
        "    for curr_run in range(runs):\n",
        "    \n",
        "        session.run(tf.global_variables_initializer())\n",
        "\n",
        "        ckpt = tf.train.get_checkpoint_state(MODEL_PATH)\n",
        "        if ckpt and ckpt.model_checkpoint_path:\n",
        "            print(ckpt.model_checkpoint_path)\n",
        "            saver.restore(session, ckpt.model_checkpoint_path) # restore all variables\n",
        "\n",
        "        # Train\n",
        "        for i in range(training_epochs):\n",
        "            loss_total = 0\n",
        "            loss_total_valid = 0\n",
        "            for batch_data, batch_labels, batch_seq_len, batch_sequence_lengths in batch(x_orig, y_orig, sequence_length_orig, batch_size, input_size):\n",
        "                tf_viterbi_sequence, tf_loss, _ = session.run([viterbi_sequence, loss, train_op], \n",
        "                                                     feed_dict={input_data: batch_data,\n",
        "                                                                labels: batch_labels, \n",
        "                                                                batch_sequence_length: batch_seq_len,\n",
        "                                                                original_sequence_lengths: batch_sequence_lengths })\n",
        "                \n",
        "                loss_total += tf_loss\n",
        "            \n",
        "\n",
        "            # Show train accuracy.\n",
        "            if i % 10 == 0:\n",
        "                # Create a mask to fix input lengths.\n",
        "                mask = (np.expand_dims(np.arange(batch_seq_len), axis=0) <\n",
        "                    np.expand_dims(batch_sequence_lengths, axis=1))\n",
        "                total_labels = np.sum(batch_sequence_lengths)\n",
        "                correct_labels = np.sum((batch_labels == tf_viterbi_sequence) * mask)\n",
        "                accuracy = 100.0 * correct_labels / float(total_labels)\n",
        "                print(\"Epoch: %d\" % i, \"Accuracy: %.2f%%\" % accuracy, \"Loss: %.6f%%\" % loss_total)\n",
        "        \n",
        "                \n",
        "                \n",
        "        # Test    \n",
        "        for batch_data, batch_labels, batch_seq_len, batch_sequence_lengths in batch(x_test, y_test, sequence_length_test, len(x_test), input_size):\n",
        "                tf_viterbi_sequence, tf_valid_loss = session.run([viterbi_sequence, loss], feed_dict={input_data: batch_data,\n",
        "                                                                labels: batch_labels, \n",
        "                                                                batch_sequence_length: batch_seq_len,\n",
        "                                                                original_sequence_lengths: batch_sequence_lengths })\n",
        "        \n",
        "        # mask to correct input sizes\n",
        "        mask = (np.expand_dims(np.arange(batch_seq_len), axis=0) <\n",
        "            np.expand_dims(batch_sequence_lengths, axis=1))\n",
        "        total_labels = np.sum(batch_sequence_lengths)\n",
        "        correct_labels = np.sum((batch_labels == tf_viterbi_sequence) * mask)\n",
        "        accuracy = 100.0 * correct_labels / float(total_labels)\n",
        "        print(\"Test accuracy: %.2f%%\" % accuracy)\n",
        "        print(\"Test Loss: %.6f%%\" % tf_valid_loss)\n",
        "\n",
        "        valid_labels, valid_predictions = get_valid_labels_predictions(batch_labels.astype(int), tf_viterbi_sequence, batch_sequence_lengths)\n",
        "        labels_df = pd.DataFrame(valid_labels)\n",
        "        predictions_df = pd.DataFrame(valid_predictions)\n",
        "        print(classification_report(valid_labels, valid_predictions, target_names=['O', 'B-ASPECT', 'I-ASPECT'], digits=3))\n",
        "        print(confusion_matrix(valid_labels, valid_predictions, labels=[0, 1, 2]))\n",
        "\n",
        "        count_errors(valid_labels, valid_predictions)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "V3tzqAWHLjaz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}